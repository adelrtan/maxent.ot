# Compare models using the likelihood ratio test
compare_models(small_model, large_model_b, method='lrt')
source("~/.active-rstudio-document", encoding = 'UTF-8', echo=TRUE)
my_aicc <- calculate_aic_c <- function(model) {
# If 3rd term's denominator is -ve
if ((model$n - model$k - 1) < 0) {
# Replace with 0
value <- calculate_aic(model) + (2 * model$k^2 + 2 * model$k) / 0
}
# Else use actual value for 3rd term's denominator
else {
value <- calculate_aic(model) + (2 * model$k^2 + 2 * model$k) / (model$n - model$k - 1)
}
return(value)
}
large_data <- system.file(
"extdata", "sample_data_file_large.txt", package = "maxent.ot"
)
large_model <- optimize_weights(large_data)
my_aicc(large_model)
my_aicc <- calculate_aic_c <- function(model) {
# If 3rd term's denominator is -ve
if ((model$n - model$k - 1) < 0) {
print("yes")
}
# Else use actual value for 3rd term's denominator
else {
print("no")
}
return(value)
}
my_aicc(large_model)
my_aicc <- calculate_aic_c <- function(model) {
# If 3rd term's denominator is -ve
if ((model$n - model$k - 1) < 0) {
# Replace with 0
value <- (2 * model$k^2 + 2 * model$k) / 0
}
# Else use actual value for 3rd term's denominator
else {
value <- (2 * model$k^2 + 2 * model$k) / (model$n - model$k - 1)
}
return(value)
}
my_aicc(large_model)
calculate_aic <- function(model) {
return(2 * model$k - 2 * model$loglik)
}
calculate_aic_c <- function(model) {
# If 3rd term's denominator is -ve
if ((model$n - model$k - 1) < 0) {
# Replace with 0
value <- calculate_aic(model) + (2 * model$k^2 + 2 * model$k) / 0
}
# Else use actual value for 3rd term's denominator
else {
value <- calculate_aic(model) + (2 * model$k^2 + 2 * model$k) / (model$n - model$k - 1)
}
return(value)
}
calculate_aic_c(large_data)
calculate_aic(large_model)
calculate_aic_c <- function(model) {
# If 3rd term's denominator is -ve
if ((model$n - model$k - 1) < 0) {
# Replace with 0
value <- calculate_aic(model) + (2 * model$k^2 + 2 * model$k) / 0
}
# Else use actual value for 3rd term's denominator
else {
value <- calculate_aic(model) + (2 * model$k^2 + 2 * model$k) / (model$n - model$k - 1)
}
return(value)
}
calculate_aic_c(large_model)
# Problematic data set: all models have n-k <= 1
# Compare models using AIC-C
compare_models(small_model, large_model, method='aic_c')
calculate_aic_c(large_model)
View(calculate_aic)
library(devtools)
setwd("C:/Users/Adeline/Documents/GitHub")
load_all("maxent.ot")
library(devtools)
setwd("C:/Users/Adeline/Documents/GitHub")
load_all("maxent.ot")
rm(list = c("calculate_aic", "calculate_aic_c"))
library(devtools)
setwd("C:/Users/Adeline/Documents/GitHub")
load_all("maxent.ot")
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>",
warning=FALSE
)
# Import packages
library(maxent.ot)
library(ggplot2)    # For plots
knitr::include_graphics("../man/figures/OT_tableau.JPG")
knitr::include_graphics("../man/figures/maxEnt_tableau_5.jpg")
knitr::include_graphics("../man/figures/MaxEnt_vs_penalty.jpg")
# This creates the .jpg image for the graph in the chunk above.
# Ideally, use this (i.e. create graph rather than use an image)...
# ...but this currently renders small plot & ugly big spaces for labels
# Hack to set up window
curve(exp(-x), from=0, to=5,
col="white", xlab="penalty", ylab="MaxEnt score")
# Add horizontal & vertical lines at y=0 & x=0
abline(h=0, col="gray")
abline(v=0, col="gray")
# Extend curve to to R-edge of window
curve(exp(-x), from=0, to=5.25, col="blue", add=TRUE)
# Get paths to sample data file
data_file <- system.file(
"extdata", "sample_data_file.txt", package = "maxent.ot"
)
# Take a look at the structure of the sample data
read.table(data_file, header=FALSE, sep='\t')
# Fit weights to data
simple_model <- optimize_weights(data_file)
# View the model we've fit (no biases used)
simple_model
# Get learned weights of model we've fit (no biases used)
simple_model$weights
# Get log likelihood of model we've fit (no biases used)
simple_model$loglik
knitr::include_graphics("../man/figures/logLike_table.jpg")
# Get paths to toy data and bias files.
data_file <- system.file(
"extdata", "sample_data_file.txt", package = "maxent.ot"
)
bias_file <- system.file(
"extdata", "sample_bias_file.txt", package = "maxent.ot"
)
# Take a look at the structure of the bias_file
read.table(bias_file, header=FALSE, sep='\t')
# Fit weights with biases specified in file
optimize_weights(data_file, bias_file)
# Fit weights with biases specified in vector form
optimize_weights(
data_file, mu_vector = c(1, 2), sigma_vector = c(100, 200)
)
# Fit weights with biases specified as scalars
optimize_weights(
data_file, mu_scalar = 0, sigma_scalar = 1000
)
# Fit weights with a mix of scalar and vector biases
optimize_weights(
data_file, mu_vector = c(1, 2), sigma_scalar = 1000
)
knitr::include_graphics("../man/figures/bias_param_hierarchy.jpg")
# Get log likelihood of simple model
simple_model$loglik
# View the data that was used to train `simple_model`
read.table(data_file, header=FALSE, sep='\t')
# Train regularized model with mu_scalar=0 & sigma_scalar=1
regularized_model <- optimize_weights(
data_file, mu_scalar=0, sigma_scalar=1
)
# Take a took at the regularized model trained with scalar biases
regularized_model
# View the bias parameters we used during model training
regularized_model$bias_params
# Get learned weights for the regularized model trained with scalar biases
regularized_model$weights
# Calculate log likelihood of regularized model trained with scalar biases
# Use regularized model's weights to get predicted probabilities
predict_regularizedModel <- predict_probabilities(
data_file, regularized_model$weights
)
# Retrieve just the predicted probabilities
pred_probs_regM <- predict_regularizedModel[, 'Predicted Probability']
# Calculate the actual log likelihood
# Since only the first three data points are observed...
# ...and each only occurs once...
# ...this is a quick way to calculate log likelihood
actual_logLike <- 0
for (i in 1:3) {
actual_logLike = actual_logLike + log(as.numeric(pred_probs_regM[i]))
}
# The log likelihood is...
print(
paste("The log likelihood of data under weights of regularized model is:",
sprintf(actual_logLike, fmt = '%#.3f'))
)
# Get value of objective function for trained regularized model
regularized_model$loglik
# Calculate value of the bias term for trained regularized model
bias_term <- actual_logLike - regularized_model$loglik
bias_term
# Get weight of Constraint 1 (simple model)
cons1_noBias <- simple_model$weights[1]
# Get weight of Constraint 1 (regularized model)
cons1_simpleBias <- regularized_model$weights[1]
# Compare learned weights of Constraint 1 in the unbiased model...
# ...and in the regularized one
print(paste("In the unbiased model, Constraint 1's  weight is:",
sprintf(cons1_noBias, fmt = '%#.3f')))
print(paste("In the regularized model, Constraint 1's  weight is:",
sprintf(cons1_simpleBias, fmt = '%#.3f')))
# Create matrix with 3 columns
tab <- matrix(c(95, 90, 73, 20), ncol=2, byrow=TRUE)
# Define column names and row names of matrix
colnames(tab) <- c('Expt cdn', 'Control cdn')
rownames(tab) <- c('trained', 'untrained')
# Convert matrix to table
tab <- as.table(tab)
# View table
tab
# Get path to data file (experimental condition)
white_salt_data_file <- system.file(
"extdata", "sample_whiteSalt_data_file.txt", package = "maxent.ot"
)
# View training data (experimental condition)
read.table(white_salt_data_file, header=FALSE, sep='\t')
# Fit model with preferred weights for experimental condition
white_salt_bias_model = optimize_weights(
white_salt_data_file, mu_vector=c(0, 0, 1.3, 3.65), sigma_scalar=sqrt(0.6)
)
# View trained weights (expt cdn with bias)
white_salt_bias_model$weights
# Fit unbiased model for experimental condition
white_salt_noBias_model = optimize_weights(
white_salt_data_file,
mu_vector=c(0, 0, 2.27, 2.27), sigma_scalar=sqrt(0.6)
)
# View trained weights (expt cdn, no bias)
white_salt_noBias_model$weights
# Get path to test file (experimental condition)
white_salt_test_file <- system.file(
"extdata", "sample_whiteSalt_test_file.txt", package = "maxent.ot"
)
# View test data (experimental condition)
read.table(white_salt_test_file, header=FALSE, sep='\t')
# Predict probabilities with weights trained with bias (expt cdn)
predict_probabilities(
white_salt_test_file, white_salt_bias_model$weights
)
# Predict probabilities with weights trained without bias (expt cdn)
predict_probabilities(
white_salt_test_file, white_salt_noBias_model$weights
)
# Create matrix with 3 columns
tab <- matrix(c(91, 93, 78, 45), ncol=2, byrow=TRUE)
# Define column names and row names of matrix
colnames(tab) <- c('Biased model', 'Unbiased model')
rownames(tab) <- c('Trained (95%)', 'Untrained (73%)')
# Convert matrix to table
tab <- as.table(tab)
# View table
print("Predicted % of alternation during response phase (experimental condition)")
tab
# Get path to data file (control condition)
white_control_data_file <- system.file(
"extdata", "sample_whiteCtrl_data_file.txt", package = "maxent.ot"
)
# View training data (control condition)
read.table(white_control_data_file, header=FALSE, sep='\t')
# Fit model with preferred weights for control condition
white_control_bias_model = optimize_weights(
white_control_data_file,
mu_vector=c(0, 0, 1.3, 3.65), sigma_scalar=sqrt(0.6)
)
# View trained weights (control cdn with bias)
white_control_bias_model$weights
# Fit unbiased model for control condition
white_control_noBias_model = optimize_weights(
white_control_data_file,
mu_vector=c(0, 0, 2.27, 2.27), sigma_scalar=sqrt(0.6)
)
# View trained weights (control cdn, no bias)
white_control_noBias_model$weights
# Get path to test file (control condition)
white_control_test_file <- system.file(
"extdata", "sample_whiteCtrl_test_file.txt", package = "maxent.ot"
)
# View test data (control condition)
read.table(white_control_test_file, header=FALSE, sep='\t')
# Predict probabilities with weights trained with bias (control cdn)
predict_probabilities(
white_control_test_file, white_control_bias_model$weights
)
# Predict probabilities with weights trained with bias (control cdn)
predict_probabilities(
white_control_test_file, white_control_noBias_model$weights
)
# Create matrix with 3 columns
tab <- matrix(c(87, 87, 26, 61), ncol=2, byrow=TRUE)
# Define column names and row names of matrix
colnames(tab) <- c('Biased model', 'Unbiased model')
rownames(tab) <- c('Trained (90%)', 'Untrained (20%)')
# Convert matrix to table
tab <- as.table(tab)
# View table
print("Predicted % of alternation during response phase (control condition)")
tab
# Get paths to toy data file
data_file_a <- system.file(
"extdata", "sample_data_file.txt", package="maxent.ot"
)
# Fit weights to data (no biases)
fit_model_a <- optimize_weights(data_file_a)
# Predict probabilities for the same input
predict_probabilities(data_file_a, fit_model_a$weights)
# Data has repeated URs (absolute frequency)
# Get paths to toy data file
data_file_b <- system.file(
"extdata", "sample_data_file_double_aFreq.txt", package="maxent.ot"
)
# Take a look at the structure of the sample data with duplicate URs
read.table(data_file_b, header=FALSE, sep='\t')
# Here's the structure of the same data without duplicate URs
read.table(data_file_a, header=FALSE, sep='\t')
# Fit weights to data (no biases)
fit_model_b <- optimize_weights(data_file_b)
# Predict probabilities for the same input (duplicate URs)
predict_probabilities(data_file_b, fit_model_b$weights)
# Get predictions with User-chosen constraint weights
# Make a list of weights
# Be sure to order the weights in exactly the same order...
# ...in which their respective constraints appear in the input file!
my_wt_ls <- list(1.5, 2.5)
# Convert to double object
my_wts <- as.double(my_wt_ls)
# Get predictions
predict_probabilities(data_file_a, my_wts)
# Get paths to toy data file
data_file_c <- system.file(
"extdata", "sample_data_file_2.txt", package="maxent.ot"
)
# Get predictions T=1
# We'll let temperature default to 1
t1_pred <- predict_probabilities(data_file_c, my_wts)
# Get predictions T=3
t3_pred <- predict_probabilities(data_file_c, my_wts, temperature=3)
# Get predictions T=5
t5_pred <- predict_probabilities(data_file_c, my_wts, temperature=5)
# View predicted probability of t1_pred
t1_pred[, 'Predicted Probability']
# Select columns we want, and store them in lettered variables
a <- t1_pred[, 'UR']
b <- t1_pred[, 'SR']
c <- t1_pred[, 'Predicted Probability']
d <- t3_pred[, 'Predicted Probability']
e <- t5_pred[, 'Predicted Probability']
# Join variables to create data frame
temperature_df <- data.frame(a, b, c, d, e)
# Rename columns
names(temperature_df) <- c('UR', 'SR', 'T=1', 'T=3', 'T=5')
# View the data frame
temperature_df
# Create matrix with 3 columns
tab <- matrix(c(73, 27, 58, 42, 55, 45), ncol=2, byrow=TRUE)
# Define column names and row names of matrix
colnames(tab) <- c('Output1-1 (%)', 'Output1-2 (%)')
rownames(tab) <- c('T=1', 'T=3', 'T-5')
# Convert matrix to table
tab <- as.table(tab)
# View table
tab
# Create separate data frames for Input1 and Input2
ur1_df <- temperature_df[1:2, 3:5]
ur2_df <- temperature_df[3:5, 3:5]
# Plot distribution of outputs for Input1
par(mar=c(5, 4, 4, 4), xpd=TRUE)
barplot(as.matrix(ur1_df),
main = "Input 1",
xlab = "Temperature",
ylab = "% output",
axes = TRUE,
legend.text = c("Out1-1", "Out1-2"),
args.legend = list(x = "topright",
inset = c(-0.55, 0),    # Move legend outside plot
cex = .7)               # Legend font size
)
# Plot distribution of outputs for Input2
par(mar=c(5, 4, 4, 4), xpd=TRUE)
barplot(as.matrix(ur2_df),
main = "Input 2",
xlab = "Temperature",
ylab = "% output",
axes = TRUE,
legend.text = c("Out2-1", "Out2-2", "Out2-3"),
args.legend = list(x = "topright",
inset = c(-0.55, 0),
cex = .7)
)
# Get paths to sample Hungarian wug data
hu_data <- system.file(
"extdata", "sample_hu_wug.txt", package = "maxent.ot"
)
# We'll use the weights learned by training on the lexicon as reported by HZSL on (p.848).
lex_wts_ls <- list(5.39, 1.69, 1.46, 3.00, 4.04, 2.45, 1.08, .91, 1.73, 2.42)
# Make a named list (optional)
names(lex_wts_ls) <- list(
'AGREE(back,nonlocal)',	'AGREE(front,local)',
'AGREE(nonhigh_front,local)',	'AGREE(low_front,local)',
'AGREE(double_front,local)',	'USE_FRONT/bilabial__',
'USE_FRONT/[+cor,+son]__', 'USE_FRONT/sibilant__',
'USE_FRONT/CC__',	'USE_BACK/[C0i:C0]__'
)
# Convert to double object (required type as argument of predict_probabilites())
lex_wts <- as.double(lex_wts_ls)
# Predict probabilities with temperature set to 1.5
# Notice also that "UTF-8" is an option for encoding, which comes in useful here
hu_pred <- predict_probabilities(
hu_data, lex_wts, temperature = 1.5, encoding = "UTF-8"
)
# Let's view some of the predicted probabilities at T=1.5
# We've dropped the constraint violations for easier viewing
# Both predicted and observed probability are conditioned over URs
# e.g. Observed probability in rows 3 & 4 don't sum to 1
# because the stem [Ã©tt] appears twice in this data set.
head(hu_pred[, -4:-(ncol(hu_pred)-3)])
# For curiosity's sake, let's compare:
# With default temperature=1...
# ... predicted probability is more polarized
hu_pred_t1 <- predict_probabilities(
hu_data, lex_wts, encoding = "UTF-8"
)
head(hu_pred_t1[, -4:-(ncol(hu_pred_t1)-3)])
# Learn weights for 500 simulated wug tests
# monte_carlo_weights(hu_pred, 500)
# Get path to the saved constraint weights trained on the 500 simulated wug tests
hu_500simul_wts_path <- system.file(
"extdata", "hu_500simuls_wts.txt", package = "maxent.ot"
)
# Store constraint weights in object `hu_500simul_wts`
hu_500simul_wts <- read.table(hu_500simul_wts_path, header=TRUE, sep='\t')
hu_500simul_wts <- data.frame(hu_500simul_wts)
# Rename columns
names(hu_500simul_wts) <- c(
'AGREE(back,nonlocal)',	'AGREE(front,local)',
'AGREE(nonhigh_front,local)',	'AGREE(low_front,local)',
'AGREE(double_front,local)',	'USE_FRONT/bilabial__',
'USE_FRONT/[+cor,+son]__', 'USE_FRONT/sibilant__',
'USE_FRONT/CC__',	'USE_BACK/[C0i:C0]__'
)
# View the first 6 rows of hu_500simul_wts
# i.e. the first 6 Grammar S's
head(hu_500simul_wts)
# Get paths to toy data files
# This file has two constraints
small_data <- system.file(
"extdata", "sample_data_file_small.txt", package = "maxent.ot"
)
# This file has three constraints
large_data <- system.file(
"extdata", "sample_data_file_large.txt", package = "maxent.ot"
)
# View small_data
read.table(small_data, header=FALSE, sep='\t')
# View large_data
read.table(large_data, header=FALSE, sep='\t')
# Fit weights to both data sets
small_model <- optimize_weights(small_data)
large_model <- optimize_weights(large_data)
# Compare models using the likelihood ratio test
compare_models(small_model, large_model, method='lrt')
# Compare models using AIC
compare_models(small_model, large_model, method='aic')
# Compare models using BIC
compare_models(small_model, large_model, method='bic')
# Get paths to toy data files
# This file has 2 constraints
small_data_c <- system.file(
"extdata", "sample_data_file_small_c.txt", package = "maxent.ot"
)
# This file has 3 constraints
large_data_c <- system.file(
"extdata", "sample_data_file_large_c.txt", package = "maxent.ot"
)
# Fit weights to both data sets
small_model_c <- optimize_weights(small_data_c)
large_model_c <- optimize_weights(large_data_c)
# Compare models using AIC-C
compare_models(small_model_c, large_model_c, method='aic_c')
# Problematic data set: all models have n-k <= 1
# Compare models using AIC-C
compare_models(small_model, large_model, method='aic_c')
calculate_aic_c(large_model)
# Problematic data set: one model has n-k <= 1
# Get paths to toy data files
# This file has 2 constraints
small_data_d <- system.file(
"extdata", "sample_data_file_small_d.txt", package = "maxent.ot"
)
# This file has 3 constraints
med_data_d <- system.file(
"extdata", "sample_data_file_med_d.txt", package = "maxent.ot"
)
# This file has 4 constraints
large_data_d <- system.file(
"extdata", "sample_data_file_large_d.txt", package = "maxent.ot"
)
# Fit weights for all data sets
small_model_d <- optimize_weights(small_data_d)
med_model_d <- optimize_weights(med_data_d)
large_model_d <- optimize_weights(large_data_d)
# Compare models using AIC-C
compare_models(small_model_d, med_model_d, large_model_d, method='aic_c')
# View problematic data set
# sample size: n = 3
# maximum number of constraints for valid aic_c score: 3-2 = 1
# actual maximum number of constraints used: 3
read.table(large_data, header=FALSE, sep='\t')
# View non-problematic data set
# sample size: n = 6
# maximum number of constraints for valid aic_c score: 6-2 = 4
# actual maximum number of constraints used: 3
read.table(large_data_b, header=FALSE, sep='\t')
rm(list=ls())
